{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSI Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numeric Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply NTILE to baseline dataset (already done)\n",
    "window_spec = Window.orderBy(F.col(continuous_field))\n",
    "baseline_binned = baseline_df.withColumn(\n",
    "    \"bin\", F.ntile(VDI_N_BINS).over(window_spec)\n",
    ")\n",
    "\n",
    "# Get the max and min values from the baseline (these will be used for outlier bins)\n",
    "max_value_in_baseline = baseline_binned.agg(F.max(continuous_field)).collect()[0][0]\n",
    "min_value_in_baseline = baseline_binned.agg(F.min(continuous_field)).collect()[0][0]\n",
    "\n",
    "# Step 2: Summarize baseline bins (already done)\n",
    "baseline_summary = (\n",
    "    baseline_binned.groupBy(\"bin\")\n",
    "    .agg(\n",
    "        F.min(continuous_field).alias(\"MIN_VALUE\"),\n",
    "        F.max(continuous_field).alias(\"MAX_VALUE\"),\n",
    "        F.count(\"*\").alias(\"n_baseline\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Handle extreme values in the validation dataset (both higher and lower than baseline range)\n",
    "validation_with_bins = validation_df.join(\n",
    "    baseline_summary,\n",
    "    (F.col(continuous_field) >= F.col(\"MIN_VALUE\")) & \n",
    "    (F.col(continuous_field) <= F.col(\"MAX_VALUE\")),\n",
    "    \"left\",\n",
    ").withColumn(\n",
    "    \"bin\",\n",
    "    F.when(F.col(continuous_field) > max_value_in_baseline, VDI_N_BINS)  # Assign to upper outlier bin\n",
    "    .when(F.col(continuous_field) < min_value_in_baseline, 0)  # Assign to lower outlier bin (0 bin)\n",
    "    .otherwise(F.col(\"bin\"))  # Otherwise assign based on normal binning\n",
    ")\n",
    "\n",
    "# Step 4: Summarize the validation bins (already done)\n",
    "validation_with_bins_summary = validation_with_bins.groupBy(\"bin\").agg(\n",
    "    F.count(\"*\").alias(\"n_validation\")\n",
    ")\n",
    "\n",
    "# Step 5: Calculate VDI metrics (already done)\n",
    "vdi_table = (\n",
    "    baseline_summary.join(validation_with_bins_summary, \"bin\", \"outer\")\n",
    "    .fillna(0, subset=[\"n_baseline\", \"n_validation\"])\n",
    "    .withColumn(\"p_baseline\", F.col(\"n_baseline\") / F.sum(\"n_baseline\").over(Window.partitionBy()))\n",
    "    .withColumn(\"p_validation\", F.col(\"n_validation\") / F.sum(\"n_validation\").over(Window.partitionBy()))\n",
    "    .withColumn(\"Difference\", F.col(\"p_baseline\") - F.col(\"p_validation\"))\n",
    "    .withColumn(\"Ratio\", F.when(F.col(\"p_validation\") != 0, F.col(\"p_baseline\") / F.col(\"p_validation\")).otherwise(0))\n",
    "    .withColumn(\"Weight_of_Evidence\", F.when(F.col(\"Ratio\") > 0, F.log(F.col(\"Ratio\"))).otherwise(0))\n",
    "    .withColumn(\"Contribution\", F.col(\"Difference\") * F.col(\"Weight_of_Evidence\"))\n",
    ")\n",
    "\n",
    "# Step 6: Calculate total VDI/CSI (already done)\n",
    "total_vdi = vdi_table.agg(F.sum(\"Contribution\").alias(\"VDI\")).collect()[0][\"VDI\"]\n",
    "\n",
    "# Step 7: Display the results (already done)\n",
    "print(\"VDI (Continuous):\", total_vdi)\n",
    "vdi_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Apply NTILE to baseline dataset\n",
    "window_spec = Window.orderBy(F.col(continuous_field))\n",
    "baseline_binned = baseline_df.withColumn(\n",
    "    \"bin\", F.ntile(VDI_N_BINS).over(window_spec)\n",
    ")\n",
    "\n",
    "# Get the max and min values from the baseline (these will be used for outlier bins)\n",
    "max_value_in_baseline = baseline_binned.agg(F.max(continuous_field)).collect()[0][0]\n",
    "min_value_in_baseline = baseline_binned.agg(F.min(continuous_field)).collect()[0][0]\n",
    "\n",
    "# Step 2: Summarize baseline bins\n",
    "baseline_summary = (\n",
    "    baseline_binned.groupBy(\"bin\")\n",
    "    .agg(\n",
    "        F.min(continuous_field).alias(\"MIN_VALUE\"),\n",
    "        F.max(continuous_field).alias(\"MAX_VALUE\"),\n",
    "        F.count(\"*\").alias(\"n_baseline\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Handle extreme values in the validation dataset (both higher and lower than baseline range)\n",
    "validation_with_bins = validation_df.join(\n",
    "    baseline_summary,\n",
    "    (F.col(continuous_field) >= F.col(\"MIN_VALUE\")) & \n",
    "    (F.col(continuous_field) <= F.col(\"MAX_VALUE\")),\n",
    "    \"left\",\n",
    ").withColumn(\n",
    "    \"bin\",\n",
    "    F.when(F.col(continuous_field) > max_value_in_baseline, VDI_N_BINS)  # Upper outlier bin\n",
    "    .when(F.col(continuous_field) < min_value_in_baseline, 0)  # Lower outlier bin\n",
    "    .otherwise(F.col(\"bin\"))  # Normal binning\n",
    ")\n",
    "\n",
    "# Step 4: Add interval column (e.g., \"0-10\")\n",
    "validation_with_bins = validation_with_bins.join(\n",
    "    baseline_summary,\n",
    "    \"bin\",\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"interval\",\n",
    "    F.concat_ws(\"-\", F.col(\"MIN_VALUE\"), F.col(\"MAX_VALUE\"))  # Create interval as \"MIN_VALUE-MAX_VALUE\"\n",
    ")\n",
    "\n",
    "# Step 5: Summarize the validation bins\n",
    "validation_with_bins_summary = validation_with_bins.groupBy(\"bin\").agg(\n",
    "    F.count(\"*\").alias(\"n_validation\")\n",
    ")\n",
    "\n",
    "# Step 6: Calculate VDI metrics\n",
    "vdi_table = (\n",
    "    baseline_summary.join(validation_with_bins_summary, \"bin\", \"outer\")\n",
    "    .fillna(0, subset=[\"n_baseline\", \"n_validation\"])\n",
    "    .withColumn(\"p_baseline\", F.col(\"n_baseline\") / F.sum(\"n_baseline\").over(Window.partitionBy()))\n",
    "    .withColumn(\"p_validation\", F.col(\"n_validation\") / F.sum(\"n_validation\").over(Window.partitionBy()))\n",
    "    .withColumn(\"Difference\", F.col(\"p_baseline\") - F.col(\"p_validation\"))\n",
    "    .withColumn(\"Ratio\", F.when(F.col(\"p_validation\") != 0, F.col(\"p_baseline\") / F.col(\"p_validation\")).otherwise(0))\n",
    "    .withColumn(\"Weight_of_Evidence\", F.when(F.col(\"Ratio\") > 0, F.log(F.col(\"Ratio\"))).otherwise(0))\n",
    "    .withColumn(\"Contribution\", F.col(\"Difference\") * F.col(\"Weight_of_Evidence\"))\n",
    ")\n",
    "\n",
    "# Step 7: Calculate total VDI/CSI\n",
    "total_vdi = vdi_table.agg(F.sum(\"Contribution\").alias(\"VDI\")).collect()[0][\"VDI\"]\n",
    "\n",
    "# Step 8: Display the results\n",
    "print(\"VDI (Continuous):\", total_vdi)\n",
    "vdi_table.select(\"bin\", \"interval\", \"p_baseline\", \"p_validation\", \"Difference\", \"Ratio\", \"Weight_of_Evidence\", \"Contribution\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Step 2: Define a function to calculate VDI for a single categorical variable\n",
    "def calculate_vdi_for_category(variable):\n",
    "    # Get distinct categories from both datasets\n",
    "    distinct_categories = (\n",
    "        baseline_df.select(F.col(variable)).union(validation_df.select(F.col(variable)))\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Join distinct categories with baseline counts\n",
    "    baseline_summary = (\n",
    "        distinct_categories.join(\n",
    "            baseline_df.groupBy(variable).agg(F.count(\"*\").alias(\"n_baseline\")),\n",
    "            variable,\n",
    "            \"left\"\n",
    "        )\n",
    "        .fillna(0, subset=[\"n_baseline\"])\n",
    "        .withColumn(\"p_baseline\", F.col(\"n_baseline\") / F.sum(\"n_baseline\").over(Window.partitionBy()))\n",
    "    )\n",
    "\n",
    "    # Join distinct categories with validation counts\n",
    "    validation_summary = (\n",
    "        distinct_categories.join(\n",
    "            validation_df.groupBy(variable).agg(F.count(\"*\").alias(\"n_validation\")),\n",
    "            variable,\n",
    "            \"left\"\n",
    "        )\n",
    "        .fillna(0, subset=[\"n_validation\"])\n",
    "        .withColumn(\"p_validation\", F.col(\"n_validation\") / F.sum(\"n_validation\").over(Window.partitionBy()))\n",
    "    )\n",
    "\n",
    "    # Combine baseline and validation summaries\n",
    "    vdi_table = (\n",
    "        baseline_summary.join(validation_summary, variable, \"outer\")\n",
    "        .fillna(0, subset=[\"n_baseline\", \"n_validation\"])  # Handle missing counts\n",
    "        .withColumn(\"Difference\", F.col(\"p_baseline\") - F.col(\"p_validation\"))\n",
    "        .withColumn(\"Ratio\", F.when(F.col(\"p_validation\") != 0, F.col(\"p_baseline\") / F.col(\"p_validation\")).otherwise(0))\n",
    "        .withColumn(\"Weight_of_Evidence\", F.when(F.col(\"Ratio\") > 0, F.log(F.col(\"Ratio\"))).otherwise(0))\n",
    "        .withColumn(\"Contribution\", F.col(\"Difference\") * F.col(\"Weight_of_Evidence\"))\n",
    "    )\n",
    "\n",
    "    # Calculate the total VDI\n",
    "    total_vdi = vdi_table.agg(F.sum(\"Contribution\").alias(\"VDI\")).collect()[0][\"VDI\"]\n",
    "    return total_vdi, vdi_table -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to calculate VDI for a single categorical variable\n",
    "def calculate_vdi_for_category(variable):\n",
    "    # Get distinct categories from both datasets\n",
    "    distinct_categories = (\n",
    "        baseline_df.select(F.col(variable)).union(validation_df.select(F.col(variable)))\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Join distinct categories with baseline counts\n",
    "    baseline_summary = (\n",
    "        distinct_categories.join(\n",
    "            baseline_df.groupBy(variable).agg(F.count(\"*\").alias(\"n_baseline\")),\n",
    "            variable,\n",
    "            \"left\"\n",
    "        )\n",
    "        .fillna(0, subset=[\"n_baseline\"])\n",
    "        .withColumn(\"p_baseline\", F.col(\"n_baseline\") / F.sum(\"n_baseline\").over(Window.partitionBy()))\n",
    "    )\n",
    "\n",
    "    # Join distinct categories with validation counts\n",
    "    validation_summary = (\n",
    "        distinct_categories.join(\n",
    "            validation_df.groupBy(variable).agg(F.count(\"*\").alias(\"n_validation\")),\n",
    "            variable,\n",
    "            \"left\"\n",
    "        )\n",
    "        .fillna(0, subset=[\"n_validation\"])\n",
    "        .withColumn(\"p_validation\", F.col(\"n_validation\") / F.sum(\"n_validation\").over(Window.partitionBy()))\n",
    "    )\n",
    "\n",
    "    # Combine baseline and validation summaries\n",
    "    vdi_table = (\n",
    "        baseline_summary.join(validation_summary, variable, \"outer\")\n",
    "        .fillna(0, subset=[\"n_baseline\", \"n_validation\"])  # Handle missing counts\n",
    "        .withColumn(\"Difference\", F.col(\"p_baseline\") - F.col(\"p_validation\"))\n",
    "        .withColumn(\"Ratio\", F.when(F.col(\"p_validation\") != 0, F.col(\"p_baseline\") / F.col(\"p_validation\")).otherwise(0))\n",
    "        .withColumn(\"Weight_of_Evidence\", F.when(F.col(\"Ratio\") > 0, F.log(F.col(\"Ratio\"))).otherwise(0))\n",
    "        .withColumn(\"Contribution\", F.col(\"Difference\") * F.col(\"Weight_of_Evidence\"))\n",
    "    )\n",
    "\n",
    "    # Calculate the total VDI\n",
    "    total_vdi = vdi_table.agg(F.sum(\"Contribution\").alias(\"VDI\")).collect()[0][\"VDI\"]\n",
    "    return total_vdi, vdi_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('carrier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a529c4fdc7b5aa21509fc44c4dad183accdd6d117a63deea0373918ddf0ff0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
